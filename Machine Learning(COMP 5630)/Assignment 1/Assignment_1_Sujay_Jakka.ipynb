{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FylX9RtXNvuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "README\n",
        "\n",
        "The only depedencies that are required are the following libraries: NumPy, Matplotlib, and Pandas.\n",
        "\n",
        "The notebook needs the following files to run: x_train.npy, y_train.npy, x_test.npy, y_test.npy, and Housing_data_regression.xlsx."
      ],
      "metadata": {
        "id": "6pZ6Hs_bZBpL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur03jbqo_YLv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Linear_Regression:\n",
        "  def __init__(self, data_matrix, theta, learning_rate = 0):\n",
        "    self.data_matrix = data_matrix\n",
        "    self.num_examples = np.shape(data_matrix)[0]\n",
        "    self.residuals_vector = np.zeros(self.num_examples)\n",
        "    self.theta = theta\n",
        "    self.learning_rate = learning_rate\n",
        "    self.W = np.zeros((self.num_examples, self.num_examples))\n",
        "\n",
        "  # Returns prediction when given feature vector\n",
        "  # Updates residuals vector across examples\n",
        "  def lr_prediction(self, feature_vector, target_value, example_number):\n",
        "    prediction = np.dot(self.theta, feature_vector)\n",
        "    self.residuals_vector[example_number] = target_value - prediction\n",
        "    return prediction\n",
        "\n",
        "  # Finds Mean Squared Error\n",
        "  def lr_mean_squared_error(self):\n",
        "    return np.sum(np.square(self.residuals_vector)) / self.num_examples\n",
        "\n",
        "  # Find weights for locally weighted linear regression\n",
        "  def find_weights_lw_lr(self, query_features, bandwidth_parameter):\n",
        "    for i in range(self.num_examples):\n",
        "      training_vector = self.data_matrix[i, :-1]\n",
        "      distance = np.linalg.norm(training_vector - query_features)\n",
        "      W_i = np.exp((-1 * np.square(distance)) / (2 * np.square(bandwidth_parameter)))\n",
        "      self.W[i][i] = W_i\n",
        "\n",
        "  # Gradient Descent Function for Linear Regression\n",
        "  def lr_gradient_descent(self, epochs):\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      # Shuffle my data for each epoch\n",
        "      np.random.shuffle(self.data_matrix)\n",
        "\n",
        "      # Find residuals for each example\n",
        "      for j in range(self.num_examples):\n",
        "        self.lr_prediction(self.data_matrix[j, :-1], self.data_matrix[j, -1], j)\n",
        "\n",
        "      # Derivative of the loss function with respect to theta\n",
        "      derivative_vector = np.dot(self.residuals_vector, self.data_matrix[:, :-1])\n",
        "\n",
        "      # Divide by number of examples and Multiply by Learning Rate\n",
        "      update = derivative_vector * ((2 / self.num_examples) * self.learning_rate)\n",
        "\n",
        "      # Update theta parameters\n",
        "      self.theta = np.add(self.theta, update)\n",
        "\n",
        "\n",
        "  # Gradient Descent Function for Locally Weighted Linear Regression\n",
        "  def lw_lr_gradient_descent(self, query_features, epochs, bandwidth_parameter):\n",
        "\n",
        "    # Find weights for each training example\n",
        "    self.find_weights_lw_lr(query_features, bandwidth_parameter)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      # Find residuals for each example\n",
        "      for j in range(self.num_examples):\n",
        "        self.lr_prediction(self.data_matrix[j, :-1], self.data_matrix[j, -1], j)\n",
        "\n",
        "      # Derivative of the loss function with respect to theta\n",
        "      derivative_vector = np.dot(np.dot(self.data_matrix[:, :-1].T, self.W), self.residuals_vector)\n",
        "\n",
        "      # Divide by number of examples and Multiply by Learning Rate\n",
        "      update = derivative_vector * ((2 / self.num_examples) * self.learning_rate)\n",
        "\n",
        "      # Update theta Parameters\n",
        "      self.theta = np.add(self.theta, update)\n",
        "\n",
        "  # Normal Equations Function for Locally Weighted Linear Regression\n",
        "  def lr_normal_quations(self):\n",
        "\n",
        "    A = np.dot(self.data_matrix[:, :-1].T, self.data_matrix[:, :-1])\n",
        "\n",
        "    # Find determinant to check if A has an inverse\n",
        "    determinant = np.linalg.det(A)\n",
        "\n",
        "    if determinant != 0:\n",
        "      A_inv = np.linalg.inv(A)\n",
        "      B = np.dot(A_inv, self.data_matrix[:, :-1].T)\n",
        "      self.theta = np.dot(B, self.data_matrix[:, -1])\n",
        "\n",
        "    else:\n",
        "      print(\"Matrix is singular, cannot find theta.\")\n",
        "\n",
        "\n",
        "  # Normal Equations Function for Locally Weighted Linear Regression\n",
        "  def lw_lr_normal_quations(self, query_features, bandwidth_parameter):\n",
        "\n",
        "    # Find weights for each training example\n",
        "    self.find_weights_lw_lr(query_features, bandwidth_parameter)\n",
        "\n",
        "    A = np.dot(np.dot(self.data_matrix[:, :-1].T, self.W), self.data_matrix[:, :-1])\n",
        "\n",
        "    # Find determinant to check if A has an inverse\n",
        "    determinant = np.linalg.det(A)\n",
        "\n",
        "    if determinant != 0:\n",
        "      A_inv = np.linalg.inv(A)\n",
        "      B = np.dot(A_inv, self.data_matrix[:, :-1].T)\n",
        "      C = np.dot(B, self.W)\n",
        "      self.theta = np.dot(C, self.data_matrix[:, -1])\n",
        "\n",
        "    else:\n",
        "      print(\"Matrix is singular, cannot find theta.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "mtR1tmAh9FqW",
        "outputId": "ab9ab3d6-331b-427a-c6f8-83d2ecb4f221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Training Data\n",
            "190.14893280390493\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theta Parameters\n",
            "[-0.17273994  0.76022019 -0.93693978  1.63100159 -4.50861899  1.69309372\n",
            " -0.41927821  0.84704868]\n"
          ]
        }
      ],
      "source": [
        "# Problem 1 Training\n",
        "\n",
        "# Retrieve data from training files\n",
        "x_train_data = np.load(\"x_train.npy\")\n",
        "y_train_data = np.load(\"y_train.npy\")\n",
        "\n",
        "# Number of Examples in Training\n",
        "num_examples_training = np.size(x_train_data)\n",
        "\n",
        "# Using a Polynomial Basis Function of Degree 7 for a 1-d feature vector\n",
        "# Weight vector need to be of size 8\n",
        "# Initializing Random Weights\n",
        "theta = np.random.randn(8)\n",
        "\n",
        "# Initialize Learning Rate\n",
        "learning_rate = .0000025\n",
        "\n",
        "# Keeps track of all of the features and target values across examples\n",
        "data_matrix_training = np.zeros((num_examples_training, np.size(theta) + 1))\n",
        "\n",
        "# Creates Features through Polynomial Basis Functions for Training\n",
        "for i in range(num_examples_training):\n",
        "  feature_vector = np.zeros(np.size(theta))\n",
        "  for j in range(np.size(theta)):\n",
        "    feature_vector[j] = np.power(x_train_data[i], j)\n",
        "  data_matrix_training[i, :-1] = feature_vector\n",
        "\n",
        "# Set the last column in my data matrix to the target values for each example\n",
        "data_matrix_training[:, -1] = y_train_data\n",
        "\n",
        "# Plot training data to see relationship\n",
        "plt.title(\"Training Data\")\n",
        "plt.scatter(x_train_data, y_train_data, color = \"blue\", label = \"Target Values\")\n",
        "\n",
        "# Create Linear Regression Object to conduct Linear Regression for Training\n",
        "LR_obj_training = Linear_Regression(data_matrix_training, theta, learning_rate)\n",
        "\n",
        "# Training\n",
        "LR_obj_training.lr_gradient_descent(10000)\n",
        "\n",
        "# Initialize Predictor Vector for Training\n",
        "y_predictions_training = np.zeros(num_examples_training)\n",
        "\n",
        "for i in range(num_examples_training):\n",
        "  prediction = LR_obj_training.lr_prediction(data_matrix_training[i, :-1], data_matrix_training[i, -1], i)\n",
        "  y_predictions_training[i] = prediction\n",
        "\n",
        "# Find MSE in Training\n",
        "# Plot Predictions\n",
        "print(\"MSE for Training Data\")\n",
        "print(LR_obj_training.lr_mean_squared_error())\n",
        "plt.scatter(data_matrix_training[:, 1], y_predictions_training, color = \"red\", label = \"Predicted Values\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print theta\n",
        "print(\"Theta Parameters\")\n",
        "print(LR_obj_training.theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh26q67O5VER",
        "outputId": "8e9246ca-026e-4852-833d-9c876c65ac2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Testing Data\n",
            "77.08701696486905\n"
          ]
        }
      ],
      "source": [
        "# Problem 1 Testing\n",
        "\n",
        "# Retrieve data from testing files\n",
        "x_test_data = np.load(\"x_test.npy\")\n",
        "y_test_data = np.load(\"y_test.npy\")\n",
        "\n",
        "# Number of Examples in Testing\n",
        "num_examples_testing = np.size(x_test_data)\n",
        "\n",
        "# Keeps track of all of the features and target values across examples\n",
        "data_matrix_testing = np.zeros((num_examples_testing, np.size(LR_obj_training.theta) + 1))\n",
        "\n",
        "# Creates Features through Polynomial Basis Functions for Testing\n",
        "for i in range(num_examples_testing):\n",
        "  feature_vector = np.zeros(np.size(LR_obj_training.theta))\n",
        "  for j in range(np.size(theta)):\n",
        "    feature_vector[j] = np.power(x_test_data[i], j)\n",
        "  data_matrix_testing[i, :-1] = feature_vector\n",
        "\n",
        "# Set the last column in my data matrix to the target values for each example\n",
        "data_matrix_testing[:, -1] = y_test_data\n",
        "\n",
        "# Create Linear Regression Object to conduct Linear Regression for Testing\n",
        "LR_obj_testing = Linear_Regression(data_matrix_testing, LR_obj_training.theta)\n",
        "\n",
        "# Initialize Predictor Vector for Testing\n",
        "y_predictions_testing = np.zeros(num_examples_testing)\n",
        "\n",
        "for i in range(num_examples_testing):\n",
        "  prediction = LR_obj_testing.lr_prediction(data_matrix_testing[i, :-1], data_matrix_testing[i, -1], i)\n",
        "  y_predictions_testing[i] = prediction\n",
        "\n",
        "# Find MSE in Testing\n",
        "print(\"MSE for Testing Data\")\n",
        "print(LR_obj_testing.lr_mean_squared_error())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGkAmZLTV7G2",
        "outputId": "24babe5d-3605-4cee-d67d-0b4e2ab90703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Training Data\n",
            "6730630576.291383\n",
            "Theta Parameters\n",
            "[   548.44818547 141917.48180335   9630.09068674 198100.31521405\n",
            "  44304.73665164  71253.16182289 -45574.45729358  -4324.29087868\n",
            " -62124.84955467]\n"
          ]
        }
      ],
      "source": [
        "# Problem 2 Training With all Features\n",
        "import pandas as pd\n",
        "\n",
        "# Read data from excel sheet\n",
        "df = pd.read_excel(\"Housing_data_regression.xlsx\")\n",
        "\n",
        "# Convert data into Numpy Matrix\n",
        "data = df.values\n",
        "\n",
        "# Create and populate data matrix\n",
        "data_matrix_training = np.zeros((np.shape(data)[0], np.shape(data)[1] + 1))\n",
        "data_matrix_training[:, :-2] = data[:, :-1]\n",
        "data_matrix_training[:, -2] = 1\n",
        "data_matrix_training[:, -1] = data[:, -1]\n",
        "\n",
        "# Initializing Random Weights\n",
        "# Vector has length of the features plus 1\n",
        "theta = np.random.randn(np.shape(data_matrix_training)[1] - 1)\n",
        "\n",
        "# Initialize Learning Rate\n",
        "learning_rate = .000575\n",
        "\n",
        "# Create Linear Regression Object to conduct Linear Regression for Training\n",
        "LR_obj_training = Linear_Regression(data_matrix_training, theta, learning_rate)\n",
        "\n",
        "# Training\n",
        "LR_obj_training.lr_gradient_descent(10000)\n",
        "\n",
        "# Find MSE in Training\n",
        "print(\"MSE for Training Data\")\n",
        "print(LR_obj_training.lr_mean_squared_error())\n",
        "\n",
        "# Print Weight Vector\n",
        "print(\"Theta Parameters\")\n",
        "print(LR_obj_training.theta)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhwIvEZS6Tu7",
        "outputId": "7d489c50-3a91-454f-f134-f62f5cce1686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Training Data\n",
            "12282616798.30218\n",
            "Theta Parameters\n",
            "[478405.49782438 -27219.37595541]\n"
          ]
        }
      ],
      "source": [
        "# Problem 2 Training with only the feature with the most effect\n",
        "\n",
        "# Using only Fourth Feature(Living Area) to Train\n",
        "# Fourth Feature's corresponding weight parameter has the largest magnitude\n",
        "\n",
        "# Creating data matrix for one feature\n",
        "data_matrix_one_feature = np.zeros((np.shape(data_matrix_training)[0], 3))\n",
        "data_matrix_one_feature[:, 0] = data_matrix_training[:, 3]\n",
        "data_matrix_one_feature[:, 1] = 1\n",
        "data_matrix_one_feature[:, 2] = data_matrix_training[:, -1]\n",
        "\n",
        "# Initializing Random Weights\n",
        "theta = np.random.randn(2)\n",
        "\n",
        "# Initialize Learning Rate\n",
        "learning_rate = .25\n",
        "\n",
        "# Create Linear Regression Object to conduct Linear Regression for Training\n",
        "LR_obj_training_one_feature = Linear_Regression(data_matrix_one_feature, theta, learning_rate)\n",
        "\n",
        "# Training\n",
        "LR_obj_training_one_feature.lr_gradient_descent(10000)\n",
        "\n",
        "# Find MSE in Training\n",
        "print(\"MSE for Training Data\")\n",
        "print(LR_obj_training_one_feature.lr_mean_squared_error())\n",
        "\n",
        "# Print Weight Vector\n",
        "print(\"Theta Parameters\")\n",
        "print(LR_obj_training_one_feature.theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK9IYd_6v3En",
        "outputId": "f47211ac-ddb5-4eac-8fba-922964bacdcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Training Data\n",
            "6621365486.993344\n",
            "Theta Parameters\n",
            "[144250.04211364  10317.62713278 202475.77941896  44370.42065489\n",
            "  70279.03528069 -46749.48198005  -4205.10393138 -64770.73974029]\n"
          ]
        }
      ],
      "source": [
        "# Problem 2 Using all the features except the feature with the least effect\n",
        "\n",
        "# Using all features except for the first feature(House ID)\n",
        "# Creating data matrix\n",
        "data_matrix_less_one = np.zeros((np.shape(data_matrix_training)[0], 9))\n",
        "data_matrix_less_one = data_matrix_training[:, 1:]\n",
        "\n",
        "# Initializing Random Weights\n",
        "theta = np.random.randn(8)\n",
        "\n",
        "# Initialize Learning Rate\n",
        "learning_rate = .0006\n",
        "\n",
        "# Create Linear Regression Object to conduct Linear Regression for Training\n",
        "LR_obj_training_less_one = Linear_Regression(data_matrix_less_one, theta, learning_rate)\n",
        "\n",
        "# Training\n",
        "LR_obj_training_less_one.lr_gradient_descent(10000)\n",
        "\n",
        "# Find MSE in Training\n",
        "print(\"MSE for Training Data\")\n",
        "print(LR_obj_training_less_one.lr_mean_squared_error())\n",
        "\n",
        "# Print Weight Vector\n",
        "print(\"Theta Parameters\")\n",
        "print(LR_obj_training_less_one.theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myKCEIgVCk4S",
        "outputId": "9ef41f67-31c3-4c9b-b8ba-44d7cd5113e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Test Data\n",
            "0.6332295851203943\n"
          ]
        }
      ],
      "source": [
        "# Problem 3\n",
        "# Using Locally Weighted Linear Regression to predict Test Data from Problem 1\n",
        "\n",
        "# Retrieve data from training files\n",
        "x_train_data = np.load(\"x_train.npy\")\n",
        "y_train_data = np.load(\"y_train.npy\")\n",
        "\n",
        "# Number of Examples\n",
        "num_examples_training = np.size(x_train_data)\n",
        "\n",
        "# Bandwidth Parameter\n",
        "bandwidth_parameter = .05\n",
        "\n",
        "# Keeps track of all of the features and target values across examples\n",
        "data_matrix_training = np.zeros((num_examples_training, 3))\n",
        "\n",
        "# Creates and Populates Data Matrix for Training\n",
        "data_matrix_training[:, 0] = x_train_data\n",
        "data_matrix_training[:, 1] = 1\n",
        "data_matrix_training[:, 2] = y_train_data\n",
        "\n",
        "# Retrieve data from testing files\n",
        "x_test_data = np.load(\"x_test.npy\")\n",
        "y_test_data = np.load(\"y_test.npy\")\n",
        "\n",
        "# Number of Examples\n",
        "num_examples_testing = np.size(x_test_data)\n",
        "\n",
        "mse = 0\n",
        "predictions = np.zeros(num_examples_testing)\n",
        "\n",
        "# Finds theta using Normal Equations\n",
        "# Find squared residuals for testing examples\n",
        "for i in range(num_examples_testing):\n",
        "  theta = np.random.randn(2)\n",
        "  LR_obj_lw = Linear_Regression(data_matrix_training, theta)\n",
        "\n",
        "  query_features = np.array([x_test_data[i], 1])\n",
        "\n",
        "  LR_obj_lw.lw_lr_normal_quations(query_features, bandwidth_parameter)\n",
        "  prediction = np.dot(LR_obj_lw.theta, query_features)\n",
        "\n",
        "  predictions[i] = prediction\n",
        "\n",
        "  mse += np.square(prediction - y_test_data[i])\n",
        "\n",
        "print(\"MSE for Test Data\")\n",
        "print(mse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n",
        "\n",
        "**Problem 1**\n",
        "\n",
        "I implemented linear regression for problem 1 by using a class structure. Where the class has instance variables such as a data matrix that contains all the features and target values across examples, the learning rate, and theta vector. The class also has the necessary functions to execute linear regression both through gradient descent and normal equations. The gradient descent implementation can be explained by two main functions. The first function I created called lr_prediction which basically takes the dot product between a feature vector for a example and the theta vector to find the predicted value. It also subtracts our prediction value from our target value to find our residual for that example. Then the function updates our residual vector. The residual vector keeps track of all the residuals across examples for a epoch. The second main function which is called lr_gradient_descent finds the gradient of the loss function with respect to theta using the residual vector the feature matrix of our examples. It then uses this gradient to find our update to our paramters by using the input learning rate.\n",
        "\n",
        "When I first plotted the training data, I initally though the data was non-linear and followed a cubic trend. So I used a 3rd degree basis function however the best mean squared error I could derive from this approach was 18317. Then I started experimenting with higher order polynomials and I landed on using a 7th degree polynomial. I was able to get a mean squared error of approximately 190 for my training data. The scatter plot of the predictions my model had on the training data vs the actual target values of the training data is shown on the output of the second code block. The final equation I derived is approximately y = 0.85x^7 - 0.42x^6 + 1.69x^5 - 4.51x^4 + 1.63x^3 - 0.94x^2 + 0.76x - 0.17. I then tested my model on the training data and I was able to get a mean squared error of approximately 77.\n",
        "\n",
        "\n",
        "**Problem 2**\n",
        "\n",
        "Question 1\n",
        "\n",
        "The average least squares error using a simple linear regression model was 6730630576.\n",
        "\n",
        "Question 2\n",
        "\n",
        "The factor that has the most effect on the final value is Living Area. The corresponding theta parameter to this had a magnitude of 198100 which was the largest out of all the feature variables. I could not use only this feature to predict the housing price. When using all the features I got a MSE of 6730630576, when using only the feature with the most effect I got a MSE of 12282616798.\n",
        "\n",
        "Question 3\n",
        "\n",
        "The factor that has the least effect on the final value is the first parameter which is House ID. This feature's corresponding theta parameter had a magnitude of 548. The MSE when we remove this feature improves a substantial amount. MSE with this feature is 6730630576 and the MSE without this feature is 6621365486.\n",
        "\n",
        "\n",
        "**Problem 3**\n",
        "\n",
        "Question 1\n",
        "\n",
        "I do not need any basis function when using the locally weighted approach. Without basis functions I was able to get a MSE of approximately 0.63 for the testing data which is very low.\n",
        "\n",
        "Question 2\n",
        "\n",
        "The difference with this implementation compared to the one for problem 1 was that in this implementation we are finding the optimal theta vector for each example in our testing data.\n",
        "In regular linear regression, we find the optimal theta vector across examples in our training set. In locally weighted linear regression, the way we find the optimal theta vector for a query point in our testing set is to assign weights to each of the points in our training data, where the training points closer to our query point have a larger weight. I was not able to get good results using the gradient descent approach for locally weighted linear regression as I had trouble finding the right learning rate and epoch value. I used a normal equations approach which I was able to get a MSE of approximatley 0.63 which was better than my linear regression approach with basis functions where I had a MSE of approximately 77.09.\n",
        "\n"
      ],
      "metadata": {
        "id": "QxgEqFKTZYej"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}